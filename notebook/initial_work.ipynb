{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-06T14:45:43.418830Z",
     "start_time": "2025-04-06T14:45:43.415784Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.nn import Module, ModuleList\n",
    "import torch.nn.functional as F\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T14:31:36.466389Z",
     "start_time": "2025-04-06T14:31:36.463161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, latent_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.kv_latent_proj = nn.Linear(embed_dim, latent_dim)\n",
    "        self.latent_to_kv_proj = nn.Linear(latent_dim, 2 * embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.q_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.kv_latent_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.latent_to_kv_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.out_proj.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Linear transformations\n",
    "        query = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        kv_latent = self.kv_latent_proj(x)\n",
    "\n",
    "        # Latent KV projection\n",
    "        latent_kv = self.latent_to_kv_proj(kv_latent)\n",
    "        key, value = torch.chunk(latent_kv, 2, dim=-1)\n",
    "        key = key.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attn_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_probs, value)\n",
    "\n",
    "        # Reshape and output projection\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
    "        output = self.out_proj(attn_output)\n",
    "\n",
    "        return output"
   ],
   "id": "55baa064df730ede",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T14:31:55.700341Z",
     "start_time": "2025-04-06T14:31:55.697300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class RotaryPositionalEmbeddings(nn.Module):\n",
    "\n",
    "  def __init__(self, d: int, base: int = 10_000):\n",
    "\n",
    "    super().__init__()\n",
    "    self.base = base\n",
    "    self.d = d\n",
    "    self.cos_cached = None\n",
    "    self.sin_cached = None\n",
    "\n",
    "  def _build_cache(self, x: torch.Tensor):\n",
    "\n",
    "    if self.cos_cached is not None and x.shape[0] <= self.cos_cached.shape[0]:\n",
    "      return\n",
    "\n",
    "    seq_len = x.shape[0]\n",
    "\n",
    "    theta = 1. / (self.base ** (torch.arange(0, self.d, 2).float() / self.d)).to(x.device) # THETA = 10,000^(-2*i/d) or 1/10,000^(2i/d)\n",
    "\n",
    "    seq_idx = torch.arange(seq_len, device=x.device).float().to(x.device) #Position Index -> [0,1,2...seq-1]\n",
    "\n",
    "    idx_theta = torch.einsum('n,d->nd', seq_idx, theta)  #Calculates m*(THETA) = [ [0, 0...], [THETA_1, THETA_2...THETA_d/2], ... [seq-1*(THETA_1), seq-1*(THETA_2)...] ]\n",
    "\n",
    "    idx_theta2 = torch.cat([idx_theta, idx_theta], dim=1) # [THETA_1, THETA_2...THETA_d/2] -> [THETA_1, THETA_2...THETA_d]\n",
    "\n",
    "\n",
    "    self.cos_cached = idx_theta2.cos()[:, None, None, :] #Cache [cosTHETA_1, cosTHETA_2...cosTHETA_d]\n",
    "    self.sin_cached = idx_theta2.sin()[:, None, None, :] #cache [sinTHETA_1, sinTHETA_2...sinTHETA_d]\n",
    "\n",
    "  def _neg_half(self, x: torch.Tensor):\n",
    "\n",
    "    d_2 = self.d // 2 #\n",
    "\n",
    "    return torch.cat([-x[:, :, :, d_2:], x[:, :, :, :d_2]], dim=-1) # [x_1, x_2,...x_d] -> [-x_d/2, ... -x_d, x_1, ... x_d/2]\n",
    "\n",
    "\n",
    "  def forward(self, x: torch.Tensor):\n",
    "\n",
    "    self._build_cache(x)\n",
    "\n",
    "    neg_half_x = self._neg_half(x)\n",
    "\n",
    "    x_rope = (x * self.cos_cached[:x.shape[0]]) + (neg_half_x * self.sin_cached[:x.shape[0]]) # [x_1*cosTHETA_1 - x_d/2*sinTHETA_d/2, ....]\n",
    "\n",
    "    return x_rope"
   ],
   "id": "29a71bfe168eff22",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T14:45:48.724194Z",
     "start_time": "2025-04-06T14:45:48.718536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RMSNorm(Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.scale = dim ** 0.5\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.normalize(x, dim = -1) * self.gamma * self.scale"
   ],
   "id": "5837d22a93ac8584",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:37:14.047565Z",
     "start_time": "2025-04-06T15:37:14.040758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GatingNetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_experts):\n",
    "        super(GatingNetwork, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, input_size)\n",
    "        gates = self.linear(x)\n",
    "        # gates: (batch_size, num_experts)\n",
    "        return F.softmax(gates, dim=1)\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_experts):\n",
    "        super(MoE, self).__init__()\n",
    "        self.gating_network = GatingNetwork(input_size, num_experts)\n",
    "        self.experts = nn.ModuleList([nn.Linear(input_size, output_size) for _ in range(num_experts)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, input_size)\n",
    "        gate_weights = self.gating_network(x)\n",
    "        # gate_weights: (batch_size, num_experts)\n",
    "\n",
    "        expert_outputs = []\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            expert_outputs.append(expert(x).unsqueeze(2))\n",
    "        # expert_outputs: list of (batch_size, output_size, 1)\n",
    "\n",
    "        expert_outputs = torch.cat(expert_outputs, dim=2)\n",
    "        # expert_outputs: (batch_size, output_size, num_experts)\n",
    "\n",
    "        weighted_outputs = expert_outputs * gate_weights.unsqueeze(1)\n",
    "        # weighted_outputs: (batch_size, output_size, num_experts)\n",
    "\n",
    "        return torch.sum(weighted_outputs, dim=2)"
   ],
   "id": "aa04be270e354416",
   "outputs": [],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
